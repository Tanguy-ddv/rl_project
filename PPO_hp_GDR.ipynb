{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "from env import *\n",
    "from domain_randomization.callbacks import GDRCallback\n",
    "\n",
    "seed_value = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_devs = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.5, 0.6, 0.7, 0.8]\n",
    "std_devs_str = ','.join(map(str, std_devs))\n",
    "\n",
    "total_timesteps = 500000\n",
    "n_episodes = 1000\n",
    "\n",
    "models_dir = f\"models_NDR_seed:{seed_value}\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "results_file_path = os.path.join(models_dir, f\"mean_rewards_seed:{seed_value}.txt\")\n",
    "\n",
    "for std_dev in std_devs:\n",
    "\n",
    "    random_source_env = gym.make(NDR)\n",
    "    random_source_env.seed(seed_value)\n",
    "\n",
    "    model_dir = f\"ppo_source_NDR_ep:{total_timesteps}_stdev:{std_dev}_nEpEval:{n_episodes}_seed:{seed_value}\"\n",
    "    model_path = os.path.join(models_dir, model_dir)\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        model_standard = PPO(\"MlpPolicy\", env=random_source_env, device='cpu', verbose=0, seed=seed_value)\n",
    "        callback = GDRCallback(model_standard, delta=std_dev)\n",
    "        model_standard.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "        model_standard.save(model_path)\n",
    "\n",
    "        model = PPO.load(model_path)\n",
    "\n",
    "        test_env_target = gym.make('CustomHopper-target-v0')\n",
    "        test_env_target.seed(seed_value)\n",
    "\n",
    "        target_mean_reward, _ = evaluate_policy(model, test_env_target, n_eval_episodes=n_episodes, render=True)\n",
    "\n",
    "        with open(results_file_path, \"a\") as results_file:\n",
    "            results_file.write(f\"{std_dev} {target_mean_reward:.2f}\\n\")\n",
    "\n",
    "        print(f\"Domain Randomization with std_dev = {std_dev} :\")\n",
    "        print(f\"name of the model directory: {model_dir}\")\n",
    "        print(f'Mean reward of standard model on NRD in target test env : {target_mean_reward}\\n')\n",
    "\n",
    "std_devs_from_file = []\n",
    "results_target_from_file = []\n",
    "\n",
    "with open(results_file_path, \"r\") as results_file:\n",
    "    for line in results_file:\n",
    "        std_dev, mean_reward = map(float, line.strip().split())\n",
    "        std_devs_from_file.append(std_dev)\n",
    "        results_target_from_file.append(mean_reward)\n",
    "\n",
    "# Sort data for plotting\n",
    "std_devs_from_file, results_target_from_file = zip(*sorted(zip(std_devs_from_file, results_target_from_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(std_devs_from_file, results_target_from_file, marker='o', markersize=10, linewidth=2, label='Mean Reward', color='tab:blue')\n",
    "plt.xlabel('Standard Deviation', fontsize=15)\n",
    "plt.ylabel('Test Mean Reward', fontsize=15)\n",
    "plt.grid(True)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Save plot\n",
    "plot_path = os.path.join(models_dir, f\"plot_with_std_devs_{std_devs_str}.png\")\n",
    "plt.savefig(plot_path)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
